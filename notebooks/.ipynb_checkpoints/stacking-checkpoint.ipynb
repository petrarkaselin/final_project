{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd433cb6-e48a-45ed-98b2-6fd9a560f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70807e9c-f86e-4cdf-9078-38d981c25bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv('../data/cleaned/combined_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8bd51-d020-462b-b1bb-6f6a63859c84",
   "metadata": {},
   "source": [
    "### Model with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8884ddf1-d3eb-436e-8d0a-91c37e3c0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the KNN model from the file\n",
    "knn_model = joblib.load('../src/knn_model.pkl')\n",
    "\n",
    "# Load the Logistic Regression model from the file\n",
    "logreg_model = joblib.load('../src/logreg_model.pkl')\n",
    "\n",
    "# Load the Decision Tree model from the file\n",
    "dt_model = joblib.load('../src/dt_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cf6b3fb-449f-40b3-aae8-abcede1b41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.831\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.99      0.90       226\n",
      "         Yes       0.73      0.15      0.25        52\n",
      "\n",
      "    accuracy                           0.83       278\n",
      "   macro avg       0.78      0.57      0.58       278\n",
      "weighted avg       0.82      0.83      0.78       278\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[223   3]\n",
      " [ 44   8]]\n",
      "Cohen's Kappa for Stacking Model: 0.202\n"
     ]
    }
   ],
   "source": [
    "X1 = combined_data.drop('attrition', axis=1)\n",
    "y1= combined_data['attrition']\n",
    "\n",
    "# Identify categorical columns to be encoded\n",
    "columns_to_encode = ['business_travel', 'department', 'education_field', 'gender', 'job_role', 'marital_status', 'over_time']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the transformer for categorical columns (OneHotEncoder)\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, columns_to_encode),\n",
    "    ])\n",
    "# Apply one-hot encoding to training and test sets\n",
    "X_train_encoded1 = preprocessor.fit_transform(X_train1)\n",
    "X_test_encoded1 = preprocessor.transform(X_test1)\n",
    "estimators = [\n",
    "    ('knn', knn_model),\n",
    "    ('logreg', logreg_model),\n",
    "    ('dt', dt_model)\n",
    "]\n",
    "\n",
    "# Define the meta-model \n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create the stacking model\n",
    "stacking_model_1 = StackingClassifier(estimators=estimators, final_estimator=meta_model)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model_1.fit(X_train_encoded1, y_train1)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_stacking1 = stacking_model_1.predict(X_test_encoded1)\n",
    "\n",
    "# Evaluate the performance of the stacking model\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test1, y_pred_stacking1),3))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test1, y_pred_stacking1))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test1, y_pred_stacking1))\n",
    "kappa_stacking1 = cohen_kappa_score(y_test1, y_pred_stacking1)\n",
    "print(\"Cohen's Kappa for Stacking Model:\", round(kappa_stacking1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3eee9-0b4a-4b00-8689-5295b4e98885",
   "metadata": {},
   "source": [
    "### Model after balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b689468-164f-4e34-b71d-e07fbc8af447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance after SMOTE:\n",
      "Accuracy (SMOTE): 0.759\n",
      "Classification Report (SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.87      0.83      0.85       226\n",
      "         Yes       0.38      0.46      0.42        52\n",
      "\n",
      "    accuracy                           0.76       278\n",
      "   macro avg       0.63      0.64      0.63       278\n",
      "weighted avg       0.78      0.76      0.77       278\n",
      "\n",
      "Confusion Matrix (SMOTE):\n",
      " [[187  39]\n",
      " [ 28  24]]\n",
      "Cohen's Kappa (SMOTE): 0.267\n",
      "\n",
      "Performance after RandomUnderSampler:\n",
      "Accuracy (RandomUnderSampler): 0.712\n",
      "Classification Report (RandomUnderSampler):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.89      0.74      0.81       226\n",
      "         Yes       0.34      0.60      0.44        52\n",
      "\n",
      "    accuracy                           0.71       278\n",
      "   macro avg       0.62      0.67      0.62       278\n",
      "weighted avg       0.79      0.71      0.74       278\n",
      "\n",
      "Confusion Matrix (RandomUnderSampler):\n",
      " [[167  59]\n",
      " [ 21  31]]\n",
      "Cohen's Kappa (RandomUnderSampler): 0.262\n",
      "\n",
      "Performance after NearMiss:\n",
      "Accuracy (NearMiss): 0.486\n",
      "Classification Report (NearMiss):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.45      0.59       226\n",
      "         Yes       0.21      0.63      0.32        52\n",
      "\n",
      "    accuracy                           0.49       278\n",
      "   macro avg       0.53      0.54      0.45       278\n",
      "weighted avg       0.72      0.49      0.54       278\n",
      "\n",
      "Confusion Matrix (NearMiss):\n",
      " [[102 124]\n",
      " [ 19  33]]\n",
      "Cohen's Kappa (NearMiss): 0.048\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = combined_data.drop('attrition', axis=1)\n",
    "y = combined_data['attrition']\n",
    "\n",
    "# Identify categorical columns to be encoded\n",
    "columns_to_encode = ['business_travel', 'department', 'education_field', 'gender', 'job_role', 'marital_status', 'over_time']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the transformer for categorical columns (OneHotEncoder)\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, columns_to_encode)\n",
    "    ])\n",
    "\n",
    "# Apply one-hot encoding to training and test sets\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# Define models for stacking\n",
    "models_for_stacking = [\n",
    "    ('KNN', knn_model),\n",
    "    ('Logistic Regression', logreg_model),\n",
    "    ('Decision Tree', dt_model)\n",
    "]\n",
    "\n",
    "# Balancing Techniques\n",
    "balancing_methods = [\n",
    "    ('SMOTE', SMOTE(random_state=42)),\n",
    "    ('RandomUnderSampler', RandomUnderSampler(random_state=42)),\n",
    "    ('NearMiss', NearMiss(version=1, n_neighbors=3))\n",
    "]\n",
    "\n",
    "for method_name, balancing_method in balancing_methods:\n",
    "    print(f\"\\nPerformance after {method_name}:\")\n",
    "\n",
    "    # Clone the models for each balancing method\n",
    "    current_models_for_stacking = [(name, clone(model)) for name, model in models_for_stacking]\n",
    "\n",
    "    # Create the stacking model with the current balancing method\n",
    "    current_stacking_model = StackingClassifier(estimators=current_models_for_stacking, final_estimator=LogisticRegression(random_state=42))\n",
    "\n",
    "    # Apply balancing method to the training set\n",
    "    X_train_resampled, y_train_resampled = balancing_method.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "    # Fit the stacking model on the resampled data\n",
    "    current_stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predictions on the test set after balancing\n",
    "    y_pred_resampled = current_stacking_model.predict(X_test_encoded)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    print(f\"Accuracy ({method_name}):\", round(accuracy_score(y_test, y_pred_resampled), 3))\n",
    "    print(f\"Classification Report ({method_name}):\\n\", classification_report(y_test, y_pred_resampled))\n",
    "    print(f\"Confusion Matrix ({method_name}):\\n\", confusion_matrix(y_test, y_pred_resampled))\n",
    "\n",
    "    # Calculate Cohen's Kappa\n",
    "    kappa_resampled = round(cohen_kappa_score(y_test, y_pred_resampled), 3)\n",
    "    print(f\"Cohen's Kappa ({method_name}): {kappa_resampled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e111d-5166-4051-9493-75f333162c2c",
   "metadata": {},
   "source": [
    "### Model trained with unbalanced data shows the best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2340d8f5-8ef2-4622-b217-6e4dc6e85168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>daily_rate</th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>hourly_rate</th>\n",
       "      <th>monthly_income</th>\n",
       "      <th>monthly_rate</th>\n",
       "      <th>num_companies_worked</th>\n",
       "      <th>percent_salary_hike</th>\n",
       "      <th>years_in_current_role</th>\n",
       "      <th>years_since_last_promotion</th>\n",
       "      <th>...</th>\n",
       "      <th>job_role</th>\n",
       "      <th>job_satisfaction</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>over_time</th>\n",
       "      <th>relationship_satisfaction</th>\n",
       "      <th>stock_option_level</th>\n",
       "      <th>training_times_last_year</th>\n",
       "      <th>work_life_balance</th>\n",
       "      <th>attrition_pred</th>\n",
       "      <th>attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.716332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.263230</td>\n",
       "      <td>0.698016</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>4</td>\n",
       "      <td>Single</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.126791</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.217651</td>\n",
       "      <td>0.915991</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.910458</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>3</td>\n",
       "      <td>Single</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.924069</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.100349</td>\n",
       "      <td>0.845796</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>3</td>\n",
       "      <td>Married</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.129872</td>\n",
       "      <td>0.583688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>2</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  daily_rate  distance_from_home  hourly_rate  monthly_income  \\\n",
       "0  0.547619    0.716332            0.000000     0.914286        0.263230   \n",
       "1  0.738095    0.126791            0.250000     0.442857        0.217651   \n",
       "2  0.452381    0.910458            0.035714     0.885714        0.057093   \n",
       "3  0.357143    0.924069            0.071429     0.371429        0.100349   \n",
       "4  0.214286    0.350287            0.035714     0.142857        0.129872   \n",
       "\n",
       "   monthly_rate  num_companies_worked  percent_salary_hike  \\\n",
       "0      0.698016              0.888889             0.000000   \n",
       "1      0.915991              0.111111             0.857143   \n",
       "2      0.012007              0.666667             0.285714   \n",
       "3      0.845796              0.111111             0.000000   \n",
       "4      0.583688              1.000000             0.071429   \n",
       "\n",
       "   years_in_current_role  years_since_last_promotion  ...  \\\n",
       "0               0.266667                    0.000000  ...   \n",
       "1               0.466667                    0.090909  ...   \n",
       "2               0.000000                    0.000000  ...   \n",
       "3               0.466667                    0.272727  ...   \n",
       "4               0.133333                    0.181818  ...   \n",
       "\n",
       "                job_role job_satisfaction  marital_status over_time  \\\n",
       "0        Sales Executive                4          Single       Yes   \n",
       "1     Research Scientist                2         Married        No   \n",
       "2  Laboratory Technician                3          Single       Yes   \n",
       "3     Research Scientist                3         Married       Yes   \n",
       "4  Laboratory Technician                2         Married        No   \n",
       "\n",
       "   relationship_satisfaction stock_option_level  training_times_last_year  \\\n",
       "0                          1                  0                         0   \n",
       "1                          4                  1                         3   \n",
       "2                          2                  0                         3   \n",
       "3                          3                  0                         3   \n",
       "4                          4                  1                         3   \n",
       "\n",
       "   work_life_balance attrition_pred  attrition  \n",
       "0                  1             No        Yes  \n",
       "1                  3             No         No  \n",
       "2                  3            Yes        Yes  \n",
       "3                  3             No         No  \n",
       "4                  3             No         No  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrition_column = combined_data.pop('attrition')  \n",
    "combined_data['attrition'] = attrition_column \n",
    "# Predictions on the entire dataset\n",
    "combined_data['attrition_pred'] = stacking_model_1.predict(preprocessor.transform(X)) \n",
    "\n",
    "# Display the updated DataFrame\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceb5b101-aec5-4644-b650-d24f869d0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model\n",
    "with open('../src/stacking_model.pkl', 'wb') as file:\n",
    "    pickle.dump(stacking_model_1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfeafba8-a682-41cc-aae2-7931f17abcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset with prediction column 'attrition_pred'\n",
    "combined_data.to_csv('../data/cleaned/data_with_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
